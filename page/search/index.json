[{"content":"熟悉我的朋友可能知道我患有抑郁症，之前已经吃了几年的药了，但最近得了新冠后莫名其妙地好了，现在每天对生活都充满了希望。\n以前抑郁的时候每天都感觉生活是灰色的，没有希望，只有痛苦，看到别人的成功只会羡慕嫉妒，每天都会自怨自艾瞧不起自己，感觉自己活得很没有价值。\n本来我是一直在服用抗抑郁症药物的，以前都不敢随便停，偶尔因为买不到药而断药，抑郁症就会复发，陷入非常强烈的负面情绪中。\n11号阳了后，我开始吃布洛芬。因为害怕对肝肾产生过大的负担，所以就把治抑郁的药停了。开始几天抑郁情绪确实因为停药加重了不少，但随着新冠逐渐好转，我发现自己的抑郁症竟然莫名其妙地好了。虽然不知道是怎么回事，但我猜可能是新冠改变了体内某种激素的水平。\n抑郁症好了之后，我感觉整个生活都变得有色彩了。以前对什么事都提不起兴趣，卧室基本不收拾，朋友叫我出去旅游也不想去，没想到现在全都变了，我变得对生活充满热情，会每天起来认真收拾卧室，会认真给自己做一顿可口的饭菜，会有一些想要游览祖国大好河山的冲动。\n最近刷b站，看到一位up主“罗汉小罗”，他37岁，患有白癜风，靠卖游戏素材生存，每个月也就赚个1000多块，但他仍然勇敢地坚持生活下去，真的让我太感动了。我刚毕业时，一个月到手7800就已经非常满足了，我现在的生活方式消费水平和当时几乎没有一点变化，我也想过上和小罗一样的精神富足的生活。\n希望大家都能勇敢地面对生活，认真地活下去，还有太多的美好等待我们去发现。\n","date":"2022-12-25T16:31:30+08:00","permalink":"https://blog.boluotou.tech/post/20221225-covid-cured-my-depression/","title":"新冠治好我的精神内耗"},{"content":"\n服务搭在k8s上，起一个DaemonSet使用Fluent-Bit收集所有日志，发到Kafka（这个kafka可以是云服务商提供的，或者是自建的，但无论如何要能被本地机器访问）。 本地机器也搞一个k8s集群，先搭ElasticSearch，再由Logstash收集Kafka中的日志（有插件）插入到es里，最后由Kibana读取。 具体命令懒得写了，大家自己研究吧。\n","date":"2022-08-14T23:09:08+08:00","permalink":"https://blog.boluotou.tech/post/20220814-logging-center-architecture/","title":"一套充分利用本地机器的日志中心架构"},{"content":"最近在搭公司的离线数仓，整理出了一批很实用的组件，在这里分享下。\nspark-sql seatunnel k8s clickhouse hive metastore dolphin-scheduler 使用seatunnel+spark将数据从业务数据库导入clickhouse中，使用dolphin-scheduler调度clickhouse SQL生产dw层和ads层数据，最后再通过seatunnel将ads层数据写回业务系统数据库。hive metastore存储spark sql中的表信息。整个系统搭在k8s上，每个组件基本都有helm charts或者operator，极大地减少了部署难度。\n","date":"2022-08-07T21:18:13+08:00","permalink":"https://blog.boluotou.tech/post/20220807-offline-data-warehouse/","title":"最近搭的一套离线数仓架构"},{"content":"这是一篇迟来的2总结。不知怎么的就忘了写了，一直拖到现在。\n2021年是单调无聊的一年。\n这一年都是在做实时数仓。主要精力都花在对口径、写sql上了，技术含量几乎为零。\n三月到六月，部门搞了场战役，所有人都得996，非常累。每天下班后，我都只能躺在床上一动不动，完全不想干任何事。到了六月最后那段时间，我深陷抑郁情，每天上班和上坟一样。也是从这时起，我就想好了，以后找工作必须找一个work life balance的公司。\n也是因为加班太多，不得不转移下注意力休息下。玩了一点《空洞骑士》，立马就喜欢上了，后来打了好一段时间，那会儿下班后，一直在打。除了“前辈”和“幅光”，其他的都打过了。\n我好像每年都会有一两个月的时间完全不想学技术，下班后一点技术也不学，就是挥霍时间去休息。其实也挺好的，每天都高强度学习很难坚持下去。\n","date":"2022-06-21T22:42:56+08:00","permalink":"https://blog.boluotou.tech/post/20220621-2021-review/","title":"2021年总结"},{"content":"最近在学习 akka，踩了很多坑，这里分享给大家。\n使用 akka-stream 限制并发度 原代码如下。\ndef fetchRlCnt(pageNumbers: Seq[Int]): Future[Int] = { val futures: Seq[Future[HttpResponse]] = pageNumbers .map(page =\u0026gt; Http() .singleRequest(HttpRequest(uri = s\u0026#34;https://examples.org/mix_list/$page\u0026#34;))) Future.sequence(futures) .map(_.map(Unmarshal(_).to[MixList])) .flatMap(Future.sequence(_)) .map(_.map(_.data.rl.length).sum) } 本意是请求所有的分页内容，以为使用 singleRequest 同时请求所有的分页即可，没想到却出错了。\n(WaitingForResponseEntitySubscription)]Response entity was not subscribed after 1 second. Make sure to read the response `entity` body or call `entity.discardBytes()` on it 为什么会这样呢？\n其实 akka-http 在 singleRequest 时，针对同一个 hostname 会创建一个连接池，如果有相同域名的请求可以提升请求速度。\n但当函数参数pages足够大，超过了连接池最大并发请求数时，新进入连接池的请求就得不到处理，也就会出现超时的情况。\n如何解决呢？\n可以将请求放到 akka-stream 中，限制同时处理的请求个数。mapAsyncUnordered的第一个参数就是最大并发个数。\ndef fetchRlCnt(pageNumbers: Seq[Int]): Future[Int] = Source(pageNumbers) .map(it =\u0026gt; HttpRequest(uri = s\u0026#34;https://examples.org/mix_list/$it\u0026#34;)) .mapAsyncUnordered(5)(Http().singleRequest(_).flatMap(Unmarshal(_).to[MixList])) .map(_.data.rl.length) .runFold(0)(_ + _) 这样一来，在 akka-stream 层面做了最大并发个数的限制，HTTP 连接池也就不会超时了。\n使用 akka-http-json 代替 spray akka-http 自带的 json 库是spray，需要手动创建一个implicit JsonFormat，不是很好用。\nimport akka.http.scaladsl.server.Directives import akka.http.scaladsl.marshallers.sprayjson.SprayJsonSupport import spray.json._ // domain model final case class Item(name: String, id: Long) // collect your json format instances into a support trait: trait JsonSupport extends SprayJsonSupport with DefaultJsonProtocol { implicit val itemFormat = jsonFormat2(Item) } // use it wherever json (un)marshalling is needed class MyJsonService extends Directives with JsonSupport { val route = get { pathSingleSlash { complete(Item(\u0026#34;thing\u0026#34;, 42)) // will render as JSON } } } akka-http-json将许多 JSON 库与 akka-http 进行了集成，非常方便，我们这里选择比较易用的circe。\n\u0026#34;de.heikoseeberger\u0026#34; %% \u0026#34;akka-http-circe\u0026#34; % \u0026#34;1.37.0\u0026#34; 使用时引入必要的包即可。\nimport de.heikoseeberger.akkahttpcirce.FailFastCirceSupport._ import io.circe.generic.auto._ class StreamerRoutes(streamerRepository: ActorRef[StreamerActor.Command]) (implicit val system: ActorSystem[_]) { implicit val timeout: Timeout = Timeout.create(system.settings.config.getDuration(\u0026#34;myapp.routes.ask-timeout\u0026#34;)) def getStreamerCount: Future[StreamerActor.StreamerCount] = streamerRepository.ask(StreamerActor.QueryStreamerCount) val routes: Route = path(\u0026#34;streamerCount\u0026#34;) { get { onSuccess(getStreamerCount)(complete(_)) } } } 可以看到，circe 不需要手动创建 Format 对象，能够自动处理序列化。\n但到这里还不算完。这里的 StreamerCount 其实是 trait，有两个实现类。\nsealed trait StreamerCount final case class StreamerCountResult(datetime: Instant, count: Int) extends StreamerCount final case class StreamerCountError(error: String) extends StreamerCount circe 在序列化时，会默认将实现类的名称作为 key 放入 json 中。\n{ \u0026#34;StreamerCountResult\u0026#34;: { \u0026#34;datetime\u0026#34;: \u0026#34;2021-08-13T10:52:50.301390Z\u0026#34;, \u0026#34;count\u0026#34;: 2843 } } 这虽然保留了类型信息方便反序列化，但与外部系统进行交互时，会很显得很多余。\n想要去除这个 key 的包装，我们可以引入circe-generic-extras包。\n\u0026#34;io.circe\u0026#34; %% \u0026#34;circe-generic-extras\u0026#34; % \u0026#34;0.14.1\u0026#34; 引入io.circe.generic.extras.Configuration并进行配置，再使用import io.circe.generic.extras.auto._替换import io.circe.generic.auto._即可。代码如下。\npackage app import akka.actor.typed.scaladsl.AskPattern._ import akka.actor.typed.{ActorRef, ActorSystem} import akka.http.scaladsl.model.{ContentTypes, HttpEntity} import akka.http.scaladsl.server.Directives._ import akka.http.scaladsl.server.Route import akka.util.Timeout import de.heikoseeberger.akkahttpcirce.FailFastCirceSupport._ import io.circe.generic.extras.auto._ import io.circe.generic.extras.Configuration import scala.concurrent.Future class StreamerRoutes(streamerRepository: ActorRef[StreamerActor.Command]) (implicit val system: ActorSystem[_]) { implicit val timeout: Timeout = Timeout.create(system.settings.config.getDuration(\u0026#34;myapp.routes.ask-timeout\u0026#34;)) implicit val genDevConfig: Configuration = Configuration.default.withDiscriminator(\u0026#34;_type\u0026#34;) def getStreamerCount: Future[StreamerActor.StreamerCount] = streamerRepository.ask(StreamerActor.QueryStreamerCount) val routes: Route = path(\u0026#34;streamerCount\u0026#34;) { get { onSuccess(getStreamerCount)(complete(_)) } } } 返回的 Response 变成了我们期望的样子，类型信息保留在了_type字段上，未来反序列化时也不会问题。\n{ \u0026#34;datetime\u0026#34;: \u0026#34;2021-08-13T10:57:35.209627Z\u0026#34;, \u0026#34;count\u0026#34;: 2883, \u0026#34;_type\u0026#34;: \u0026#34;StreamerCountResult\u0026#34; } ","date":"2021-08-12T11:06:25+08:00","permalink":"https://blog.boluotou.tech/post/20210812-experience-using-akka/","title":"使用akka踩的一些坑"},{"content":"今天稍微聊点应用层网络协议设计。\n简介 众所周知，tcp是一种面向字节流的协议，可以看作一条无尽的水流。如果对水流的内容不加区分，便完全不知道各字节所代表的含义，进而无法处理。如此看来，设计一个能清晰划分字节流边界的应用层协议就显得非常必要。这便涉及到今天所讲的TLV协议。\nTLV全称Type–length–value，是大多数应用层协议的设计思路。主要内容包括\n最前面的若干位字节表明传统的是否为该协议（本文暂定为4个字节） 紧跟着若干字节（本文暂定为4个字节，也就是Java中的int），代表后续value的字节长度 最后是代表value的字节，其长度为2中所获得的数量 整个系统的状态机如图所示。\n实现 不同状态的Handler 用nio具体编写时，可以先抽象出一个Handler接口，可以接受一个byte进行处理。再添加三个实现（私有方法），代表0、1、2这三种状态对应的Handler。\ninterface Handler { int feed(byte b); } private int readType(byte b) { // TODO } private int readLength(byte b) { // TODO } private int readValue(byte b) { // TODO } 状态的定义 接着定义三种状态对应的code和一个Handler数组，这样后面可以用当前状态去取对应的Handler（handlers[state]）。state表示当前状态，初始值当然是TYPE_STATE。\nstatic final int TYPE_STATE = 0; static final int LENGTH_STATE = 1; static final int VALUE_STATE = 2; final Handler[] handlers = new Handler[]{ this::readType, this::readLength, this::readValue }; int state = TYPE_STATE; 对外接口 对外接口是read()，供外层在读事件就绪时调用。这里会不停地一个字节一个字节的读取ByteBuffer，并调用当前状态的Handler去处理。如果返回-1代表处理失败，需要关闭连接。\npublic void read() throws IOException { while (true) { int i = channel.read(buffer); if (i == -1) { close(); return; } if (i == 0) return; buffer.flip(); while (buffer.hasRemaining()) { byte b = buffer.get(); int f = handlers[state].feed(b); if (f == -1) { close(); return; } } buffer.clear(); } } 接受到的数据 接着定义若干byte[]，代表每个状态接收的字节，同时还有若干Pointer代表当前填充到第几个字节。注意，代表value的data并没有初始化，因为它的长度是根据length确定的。\nfinal byte[] type = new byte[4]; byte typePointer = 0; final byte[] length = new byte[4]; byte lengthPointer = 0; int dataLength = 0; byte[] data; readType Handler 接下来就可以实现之前提到的三个Handler了。先看readType，每次一次byte进来，它就会填充到type(byte[])的下一位，并判断是否满足长度，如果不满足则等待下一个byte。如果长度够4位了，但内容和约定的不一样，会返回-1，代表内容有误，供上层处理。如果长度和内容正确，那么状态就转移到LENGTH_STATE上，由下一个readLength Handler处理。\nprivate int readType(byte b) { type[typePointer] = b; if (typePointer != 3) { typePointer++; return 0; } typePointer = 0; if (!Arrays.equals(type, new byte[]{0x01, 0x23, 0x45, 0x67})) { return -1; } state = LENGTH_STATE; return 0; } readLength Handler readLength Handler和上面也是类似的，额外多了一步，将读取的value长度保存到dataLength字段中，并将状态转移到VALUE_STATE上。\nprivate int readLength(byte b) { length[lengthPointer] = b; if (lengthPointer != 3) { lengthPointer++; return 0; } lengthPointer = 0; dataLength = ByteBuffer.wrap(length).getInt(); if (dataLength == 0) { return -1; } data = new byte[dataLength]; state = VALUE_STATE; return 0; } readValue Handler readValue Handler将字节存储起来，直到数量和readLength中获得的dataLength值一样，之后就可以进行正常的业务逻辑处理，我这里是简单地print所有数据。\nprivate int readValue(byte b) { data[dataPointer] = b; if (dataPointer != dataLength - 1) { dataPointer++; return 0; } dataPointer = 0; process(); state = TYPE_STATE; return 0; } private void process() { if (data == null) return; String s = new String(data, StandardCharsets.UTF_8); System.out.println(s); } 总结 tlv协议的实现大致就是这样。实现一个状态机，根据输入，判断是否转移到下一步状态。本文的实现比较简单，value是作为请求的body存在的，如果需要，还可以加上Header或timeout处理之类额外的功能。\n原文代码在这里，里面除了TLV还有其他java nio的实践，欢迎交流。\n参考资料\nhttps://en.wikipedia.org/wiki/Type%E2%80%93length%E2%80%93value ","date":"2021-07-12T22:10:50+08:00","permalink":"https://blog.boluotou.tech/post/20210712-tlv-protocol/","title":"浅谈TLV协议的实现"},{"content":"工作后一直用MacBook Pro(15款和17款)，除了公司发的，自己也有一台，现在已经完全习惯了在macOS下开发。macOS下的各种工具基本都把玩了一遍，有一些非常顺手，格外喜欢，这里分享给大家。\n操作系统 已经升级到了Big Sur，但发现很多软件有兼容性问题，所以还是推荐大家用Catalina。\nCatalina 浏览器 虽然我很喜欢Firefox，但这些年的发展实在不怎么亮眼。而且Firefox还搞了中国特供版，账号系统和国际版不能兼容，登录时经常不知道到底使用了哪个版本，实在太窝囊了。还是老老实实用Chrome。\nChrome 编辑器 已经2021年了，编辑器首选vscode。虽然Sublime Text出了3.x，但用着不如vscode顺手。命令行里一般用vim。学过emacs，但不是很熟练。\nVisual studio code Vim IDE 不用想，写Java/Scala永远离不开Jetbrains家的Intellij IDEA。\nIntellij IDEA 容器 经常需要启动一些后端中间件，Docker必须有。\ndocker 终端 只有它了。\niterm2 命令行 我这几年积累了很多命令行工具，每一个都无可替代。\nohmyzsh ohmyzsh theme powerlevel10k 强烈推荐，功能很完备一个主题 ohmyzsh plugins zsh-autosuggestions 根据历史记录自动推荐命令 zsh-syntax-highlighting 命令行语法高亮，再也不用担心用错命令 fzf git-open docker fzf: 快速查找文件、进程 tmux: 终端\u0008复用工具，强烈推荐，搭配这个配置 tldr: 总结了各个命令的常见用法 nvm: 管理nodejs版本 jenv: 管理jdk版本 z: 智能目录跳转 tig: git客户端 tree: 目录树 ack: grep替代品 mosh: mosh客户端 ncdu: 查看磁盘使用情况 htop: 查看进程 diff-so-fancy: 很好的文本比对工具 字体 更纱黑体作为等宽字体，不仅适配了多国语言，也同时拥有命令行(Term)和等宽(Mono)这两种不同场景的等宽字体。\n更纱黑体 Sarasa Gothic 窗口管理 macOS的原生窗口管理功能很少，必须用其他软件加强。rectangle是一个，hammerspoon自己写配置也是一个，其他的都要付费，不是很感冒。\nRectangle 输入法 我喜欢五笔，所以并没有太多选择。\n清歌输入法 绘画 我画的是板绘，可惜windows下流行的板绘软件很多并不支持macOS，最终选择了KDE开源的Krita。\nKrita 键盘化 键盘的效率远高于鼠标，所以能用键盘完成的事情要尽可能用键盘。\nalfred4 hammerspoon（可以参考我的上一篇博文） 总结 这一套配置更侧重于命令行和键盘操作，需要配置的东西也有一些（比如hammerspoon)，但调教好了之后用起来还是很爽的。可惜macOS下好像没什么工具可以很好的操作鼠标，有用过vimac，但发现会提高输入延迟。\n","date":"2021-06-29T20:31:06+08:00","permalink":"https://blog.boluotou.tech/post/20210629-macos-softwares/","title":"我的macOS软件清单"},{"content":"macOS没有内置窗口管理功能，需要安装第三方软件来实现。常用的免费软件有Spectacle和ShiftIt，这已不再维护。今天将macOS升级到11.3后，发现ShiftIt彻底不能使用了。搜寻后在ShiftIt wiki里找到了替代品hammerspoon。\n介绍 Hammerspoon 是一款macOS平台的免费开源软件，通过桥接操作系统与 Lua 脚本引擎的方式，让我们可以通过编写 Lua 代码来实现操作应用程序、窗口、鼠标、文本、音频设备、电池、屏幕、剪切板、定位、wifi等。基本囊括了系统的各方面。\n安装 brew cask install hammerspoon 编写脚本 创建.hammerspoon目录和init.lua文件。\nmkdir ~/.hammerspoon cd ~/.hammerspoon touch init.lua 编辑init.lua，填写以下内容。\nhs.window.animationDuration = 0 units = { right50 = { x = 0.50, y = 0.00, w = 0.50, h = 1.00 }, left50 = { x = 0.00, y = 0.00, w = 0.50, h = 1.00 }, top50 = { x = 0.00, y = 0.00, w = 1.00, h = 0.50 }, bot50 = { x = 0.00, y = 0.50, w = 1.00, h = 0.50 }, maximum = { x = 0.00, y = 0.00, w = 1.00, h = 1.00 } } hs.hotkey.bind({ \u0026#39;ctrl\u0026#39;, \u0026#39;alt\u0026#39;, \u0026#39;cmd\u0026#39;}, \u0026#39;n\u0026#39;, function() local win = hs.window.focusedWindow() -- get the screen where the focused window is displayed, a.k.a. current screen local screen = win:screen() -- compute the unitRect of the focused window relative to the current screen -- and move the window to the next screen setting the same unitRect win:move(win:frame():toUnitRect(screen:frame()), screen:next(), true, 0) end) mash = { \u0026#39;ctrl\u0026#39;, \u0026#39;alt\u0026#39;, \u0026#39;cmd\u0026#39; } hs.hotkey.bind(mash, \u0026#39;right\u0026#39;, function() hs.window.focusedWindow():move(units.right50, nil, true) end) hs.hotkey.bind(mash, \u0026#39;left\u0026#39;, function() hs.window.focusedWindow():move(units.left50, nil, true) end) hs.hotkey.bind(mash, \u0026#39;up\u0026#39;, function() hs.window.focusedWindow():move(units.top50, nil, true) end) hs.hotkey.bind(mash, \u0026#39;down\u0026#39;, function() hs.window.focusedWindow():move(units.bot50, nil, true) end) hs.hotkey.bind(mash, \u0026#39;m\u0026#39;, function() hs.window.focusedWindow():move(units.maximum, nil, true) end) 这个脚本将若干功能绑定到快捷键上，包括\nctrl+alt+cmd+n - 将当前窗口移动到下一个显示器 ctrl+alt+cmd+↑ - 将当前窗口移动到屏幕上半边 ctrl+alt+cmd+↓ - 将当前窗口移动到屏幕下半边 ctrl+alt+cmd+← - 将当前窗口移动到屏幕右半边 ctrl+alt+cmd+→ - 将当前窗口移动到屏幕左半边 ctrl+alt+cmd+m - 将当前窗口最大化 配置好后并不能立即使用，还需要启动Hammerspoon。\n启动 打开Hammerspoon软件，点击Reload config，这样就可以愉快地管理窗口了。\n结论 至此，我们已经实现了窗口管理的基本功能。当然，Hammerspoon的功能远不止此，感兴趣的读者可以去Hammerspoon官网了解。\n","date":"2021-05-26T19:49:27+08:00","permalink":"https://blog.boluotou.tech/post/20210526-use-hammerspoon-to-manage-macos-window/","title":"使用hammerspoon管理macOS窗口"},{"content":"2020年是我人生中最不寻常的一年，经历了疫情、换工作两件大事。\n疫情 这个大家都经历过，不说了。\n换工作 马云有句话说的好，“员工的离职原因很多，但其实就两点：一，钱没给到位；二，心委屈了”。我就属于钱没给到位，心还委屈了的。还好拿到了一份还不错的offer。和新同事相处了一段时间，感觉他们人都还不错。\n技术 上半年主要在学AWS，因为客户在用。我其实是不太想学这个的，因为海外客户，他们的这套技术栈和国内主流技术栈大相迳庭，以后很难用到。不过硬着头皮学习后发现AWS还是很先进的。\n下半年主要学习找工作需要的国内常见的技术，包括MySQL, K8s，JVM之类的。因为新工作的关系，对学习了Flink。Flink强大又复杂，学习起来很有意思。\n生活 因为新工作的原因，一个人来到陌生的城市工作。好在之前有很多在网上认识的朋友，给予了一些生活上的指导。和其中几个面基了。\n总结 总的来说还不错，比我预想的要好很多。新的一年也要加油啊！\n","date":"2021-01-03T02:26:00+08:00","permalink":"https://blog.boluotou.tech/post/20210103-2020-review/","title":"2020年回顾"},{"content":"最近搞了一台vps，用它搭建了一个远程下载服务（网盘），包括aria2做下载工具，nginx做静态文件服务，haproxy根据hostname做代理。\n架构大致如下。\nAria2配置如下，大家基本可以直接照抄，只需要修改下rpc-secret，用aria2c --conf-path=/root/aria2/aria2.conf -D命令启动即可。\n# touch /data/aria2.session # vim /etc/aria2/aria2.conf ## \u0026#39;#\u0026#39;开头为注释内容, 选项都有相应的注释说明, 根据需要修改 ## ## 被注释的选项填写的是默认值, 建议在需要修改时再取消注释 ## ## 文件保存相关 ## # 文件的保存路径(可使用绝对路径或相对路径), 默认: 当前启动位置 dir=/root/Downloads # 启用磁盘缓存, 0为禁用缓存, 需1.16以上版本, 默认:16M disk-cache=32M # 文件预分配方式, 能有效降低磁盘碎片, 默认:prealloc # 预分配所需时间: none \u0026lt; falloc ? trunc \u0026lt; prealloc # falloc和trunc则需要文件系统和内核支持 # NTFS建议使用falloc, EXT3/4建议trunc, MAC 下需要注释此项 file-allocation=trunc # 断点续传 continue=true ## 下载连接相关 ## # 最大同时下载任务数, 运行时可修改, 默认:5 max-concurrent-downloads=10 # 同一服务器连接数, 添加时可指定, 默认:1 max-connection-per-server=10 # 最小文件分片大小, 添加时可指定, 取值范围1M -1024M, 默认:20M # 假定size=10M, 文件为20MiB 则使用两个来源下载; 文件为15MiB 则使用一个来源下载 min-split-size=10M # 单个任务最大线程数, 添加时可指定, 默认:5 split=5 # 整体下载速度限制, 运行时可修改, 默认:0 #max-overall-download-limit=0 # 单个任务下载速度限制, 默认:0 #max-download-limit=0 # 整体上传速度限制, 运行时可修改, 默认:0 #max-overall-upload-limit=0 # 单个任务上传速度限制, 默认:0 #max-upload-limit=0 # 禁用IPv6, 默认:false disable-ipv6=true ## 进度保存相关 ## # 从会话文件中读取下载任务 input-file=/root/aria2/aria2.session # 在Aria2退出时保存`错误/未完成`的下载任务到会话文件 save-session=/root/aria2/aria2.session # 定时保存会话, 0为退出时才保存, 需1.16.1以上版本, 默认:0 save-session-interval=60 ## RPC相关设置 ## pause=false rpc-allow-origin-all=true rpc-listen-all=true rpc-save-upload-metadata=true rpc-secure=false # 启用RPC, 默认:false enable-rpc=true # 允许所有来源, 默认:false #rpc-allow-origin-all=true # 允许非外部访问, 默认:false #rpc-listen-all=true # 事件轮询方式, 取值:[epoll, kqueue, port, poll, select], 不同系统默认值不同 event-poll=epoll # RPC监听端口, 端口被占用时可以修改, 默认:6800 rpc-listen-port=6800 # 设置的RPC授权令牌, v1.18.4新增功能, 取代 --rpc-user 和 --rpc-passwd 选项 rpc-secret=XXXXXX # 设置的RPC访问用户名, 此选项新版已废弃, 建议改用 --rpc-secret 选项 #rpc-user=\u0026lt;USER\u0026gt; # 设置的RPC访问密码, 此选项新版已废弃, 建议改用 --rpc-secret 选项 #rpc-passwd=\u0026lt;PASSWD\u0026gt; ## BT/PT下载相关 ## # 当下载的是一个种子(以.torrent结尾)时, 自动开始BT任务, 默认:true #follow-torrent=true # BT监听端口, 当端口被屏蔽时使用, 默认:6881-6999 listen-port=51413 # 单个种子最大连接数, 默认:55 #bt-max-peers=55 # 打开DHT功能, PT需要禁用, 默认:true enable-dht=true # 打开IPv6 DHT功能, PT需要禁用 #enable-dht6=false # DHT网络监听端口, 默认:6881-6999 #dht-listen-port=6881-6999 # 本地节点查找, PT需要禁用, 默认:false bt-enable-lpd=true # 种子交换, PT需要禁用, 默认:true enable-peer-exchange=false # 每个种子限速, 对少种的PT很有用, 默认:50K #bt-request-peer-speed-limit=50K # 客户端伪装, PT需要 #peer-id-prefix=-TR2770- user-agent=Transmission/2.92 #user-agent=netdisk;4.4.0.6;PC;PC-Windows;6.2.9200;WindowsBaiduYunGuanJia # 当种子的分享率达到这个数时, 自动停止做种, 0为一直做种, 默认:1.0 seed-ratio=1.0 #作种时间大于30分钟，则停止作种 seed-time=30 # 强制保存会话, 话即使任务已经完成, 默认:false # 较新的版本开启后会在任务完成后依然保留.aria2文件 #force-save=false # BT校验相关, 默认:true #bt-hash-check-seed=true # 继续之前的BT任务时, 无需再次校验, 默认:false bt-seed-unverified=true # 保存磁力链接元数据为种子文件(.torrent文件), 默认:false bt-save-metadata=true #下载完成后删除.ara2的同名文件 on-download-complete=/root/aria2/delete_aria2 Aria2后端搭建好后，还需要一个前端页面去操作，这里选用AriaNg，下载最新版本，置于/root/Aria2Ng目录下，同时配置nginx来host这个页面。Nginx配置如下。注意，使用了8081端口，之后还会提到它。\nserver { listen 8081; location / { autoindex on; root /root/AriaNg; } } 接着搭建静态文件服务器，在之前的aria2配置里，默认下载目录为/root/Downloads，我们只需要建一个文件下载服务器，绑定到这个目录即可。好在nginx提供了开箱即用的文件下载服务器。配置如下，端口为8080。\nserver { listen 8080; location / { autoindex on; root /root/Downloads; } } 这三个服务全都在同一台机器上，使用端口访问很不方便，所以加一个代理，根据hostname的不同代理到不同服务上。这里我用的是haproxy，haproxy是一一款老牌的proxy应用，易用又强大。核心配置如下。\nfrontend front443 bind :443 ssl crt-list /etc/haproxy/crt-list.txt alpn http/1.1 bind :80 mode http redirect scheme https code 301 if !{ ssl_fc } acl is_xswl hdr(host) -i xswl.tnb.tw acl is_yysy hdr(host) -i yysy.tnb.tw acl is_awsl hdr(host) -i awsl.tnb.tw use_backend fileserver8080 if is_xswl use_backend aria2c6800 if is_yysy use_backend AriaNg8081 if is_awsl backend fileserver8080 mode http server fileserver8080 127.0.0.1:8080 backend AriaNg8081 mode http server AriaNg8081 127.0.0.1:8081 backend aria2c6800 mode http server aria2c6800 127.0.0.1:6800 通过acl创建hostname的判断条件，如果符合给定的条件则代理到对应的后端服务上。以下是aria2 frontend与文件下载页面的截图。\n","date":"2020-12-26T00:00:00+08:00","permalink":"https://blog.boluotou.tech/post/20201226-haproxy-nginx-and-aria2/","title":"使用haproxy，nginx与aria2搭建下载服务"},{"content":"SQS 全称 Simple queue service，是 AWS 推出一款消息队列服务。按照 AWS 官方文档的说法，SQS 居有高吞吐、高可用的特性。从我个人的开发体验来看，SQS 是一款相当易用并且功能强大的消息队列服务，满足了生产使用的大部分需求。\nAmazon Simple Queue Service (SQS) 是一种完全托管的消息队列服务，可让您分离和扩展微服务、分布式系统和无服务器应用程序。SQS 消除了与管理和运营消息型中间件相关的复杂性和开销，并使开发人员能够专注于重要工作。借助 SQS，您可以在软件组件之间发送、存储和接收任何规模的消息，而不会丢失消息，并且无需其他服务即可保持可用。使用 AWS 控制台、命令行界面或您选择的 SDK 和三个简单的命令，在几分钟内即可开始使用 SQS。\n什么是指数退避？ 不过有一项功能 SQS 并没有原生实现，那就是“指数退避”。指数退避是指在消息处理失败时，消息队列会发送消息，直到消息处理成功或者超时。而针对每条消息，每次重试的间隔时间是指数级上升的，比如第一次重试 3 秒，第二次重试 9 秒，第三次重试 27 秒等等，以此类推。这个功能在系统设计中属于常见功能。\n虽然 SQS 没有原生实现，但这我们可以利用 SQS 现有 API 来实现。本文关注使用的，就是 SQS 中 VisibilityTimeout 和 ApproximateReceiveCount 属性以changeMessageVisibilityAPI。\n什么是 VisibilityTimeout VisibilityTimeout 这个概念源于 SQS 中的一个巧妙设计。\n当一个消费者接收到 SQS 中的消息后，SQS 并不会立刻将这条消息从队列中删除，因为 SQS 并不知道消费者是否成功接受并处理了消息。如果想要删除这条消息，消费者必须在处理完消息后，手动调用 SQS 的deleteMessageAPI 去删除，这样 SQS 才会认为这条消费已经被成功消费，可以从队列中删除了。\n可问题来了，如果消费者 A 接受到消息后，消息还在队列中，消费者 B 能不能同时接受并处理这条消息呢？如果经常有多个消费者同时处理同一个消息，系统不是就乱套了吗？\n基于这种考虑，SQS 对每条消息设置了一个 VisibilityTimeout 的属性，这个属性的值代表了时间长度。如果消费者 A 获得了消息，那么这条消息在 VisibilityTimeout 这么长的时间内是不会被其他消费者读取的，只有超过了这个时间长度，其他消费者才可以读取。也正是基于这个设计，大家在配置 VisibilityTimeout 时长时，要比处理这条消息所花的时间稍长一点。\n有了这个属性，还有操作它的changeMessageVisibilityAPI，我们就可以构思出指数退避的大致方案了。每当消息处理失败时，就重新设置消息的 VisibilityTimeout，并且值是上一次的 N 倍（下面全部都设定为2）。伪代码如下：\nconst message = receiveMessage(); try { process(message); } catch (e) { const retriesCount = getRetriesCount(message); changeMessageVisibility(message, 2 ^ retriesCount); } 什么是 ApproximateReceiveCount？ 上面伪代码需要从消息中获取重试次数，如何获得呢？我们可以在从 SQS 接受消息时，指定额外的Attributes字段告知 SQS 返回相关属性。\n请求\nhttps://sqs.us-east-2.amazonaws.com/xxxxxxxxxxx/MyQueue/ ?Action=ReceiveMessage \u0026amp;AttributeName=All 响应\n\u0026lt;ReceiveMessageResponse\u0026gt; \u0026lt;ReceiveMessageResult\u0026gt; \u0026lt;Message\u0026gt; \u0026lt;MessageId\u0026gt;5fea7756-0ea4-451a-a703-a558b933e274\u0026lt;/MessageId\u0026gt; \u0026lt;ReceiptHandle\u0026gt; MbZj6wDWli+JvwwJaBV+3dcjk2YW2vA3+STFFljTM8tJJg6HRG6PYSasuWXPJB+Cw Lj1FjgXUv1uSj1gUPAWV66FU/WeR4mq2OKpEGYWbnLmpRCJVAyeMjeU5ZBdtcQ+QE auMZc8ZRv37sIW2iJKq3M9MFx1YvV11A2x/KSbkJ0= \u0026lt;/ReceiptHandle\u0026gt; \u0026lt;MD5OfBody\u0026gt;fafb00f5732ab283681e124bf8747ed1\u0026lt;/MD5OfBody\u0026gt; \u0026lt;Body\u0026gt;This is a test message\u0026lt;/Body\u0026gt; \u0026lt;Attribute\u0026gt; \u0026lt;Name\u0026gt;SenderId\u0026lt;/Name\u0026gt; \u0026lt;Value\u0026gt;195004372649\u0026lt;/Value\u0026gt; \u0026lt;/Attribute\u0026gt; \u0026lt;Attribute\u0026gt; \u0026lt;Name\u0026gt;SentTimestamp\u0026lt;/Name\u0026gt; \u0026lt;Value\u0026gt;1238099229000\u0026lt;/Value\u0026gt; \u0026lt;/Attribute\u0026gt; \u0026lt;Attribute\u0026gt; \u0026lt;Name\u0026gt;ApproximateReceiveCount\u0026lt;/Name\u0026gt; \u0026lt;Value\u0026gt;5\u0026lt;/Value\u0026gt; \u0026lt;/Attribute\u0026gt; \u0026lt;Attribute\u0026gt; \u0026lt;Name\u0026gt;ApproximateFirstReceiveTimestamp\u0026lt;/Name\u0026gt; \u0026lt;Value\u0026gt;1250700979248\u0026lt;/Value\u0026gt; \u0026lt;/Attribute\u0026gt; \u0026lt;/Message\u0026gt; \u0026lt;/ReceiveMessageResult\u0026gt; \u0026lt;ResponseMetadata\u0026gt; \u0026lt;RequestId\u0026gt;b6633655-283d-45b4-aee4-4e84e0ae6afa\u0026lt;/RequestId\u0026gt; \u0026lt;/ResponseMetadata\u0026gt; \u0026lt;/ReceiveMessageResponse\u0026gt; 返回的响应即含有ApproximateReceiveCount，它就是伪代码中的retriesCount。\n小结 本文基于 SQS 已有的 API，构建出一种简单的指数退避方案。除了已提到的内容，还要注意系统的幂等性，避免多次处理消息失败产生额外的影响。\n参考资料 https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html ","date":"2020-08-08T16:35:20+08:00","permalink":"https://blog.boluotou.tech/post/20200808-exponential-backoff-in-sqs/","title":"AWS SQS如何实现指数退避"},{"content":"最近在思考node.js如何做服务间与服务内部的日志追踪，一个很简单的实现就是在HTTP request header里添加一个字段x-trace-id来标识唯一性，打印日志时添加x-trace-id的值。但如何保存这个状态呢？\n我看到的一些框架和应用会将traceId作为参数传递到应用的各个函数/方法里，调用logger.info(traceId, \u0026quot;XXX\u0026quot;)来实现打印traceId的功能。这样的写法并不优雅，但node.js又不像其他语言框架，比如Spring MVC，可以用过ThreadLocal来保存这个状态，那如何保存这个状态呢？其实node.js已经有了这样一个API：AsyncLocalStorage。\nAsyncLocalStorage属于async_hooks模块，node.js v14引入，后被反向移植到v12上。下面用一个express应用来说明它的用法。\nimport { AsyncLocalStorage } from \u0026#34;async_hooks\u0026#34; import express from \u0026#34;express\u0026#34; import { v4 as uuid4 } from \u0026#34;uuid\u0026#34; export const app = express() const t = new AsyncLocalStorage\u0026lt;string\u0026gt;() app.use((req, _, next) =\u0026gt; { const traceId = req.header(\u0026#39;x-trace-id\u0026#39;) t.run(traceId ?? uuid4(), next) }) app.get(\u0026#39;/\u0026#39;, async function (_, res) { console.log(\u0026#39;start\u0026#39;, t.getStore()) // 模拟耗时操作 await new Promise((resolve) =\u0026gt; { setTimeout(() =\u0026gt; resolve(), 100) }) console.log(\u0026#39;end\u0026#39;, t.getStore()) res.send(\u0026#39;Hello World!\u0026#39;) }) app.listen(3000) 这里我们写了一个middleware，将HTTP Request Header里的信息保存到AsyncLocalStorage实例中，需要打印日志时使用getStore方法获取。\n我们期望的行为是：一个请求进来后，保存并打印它的traceId，之后进行了一些异步操作，操作结束后，我们能通过AsyncLocalStorage获取到保存的traceId。下面我们就使用ab命令验证一下。\nab -n 6 -c 2 http://localhost:3000/ 从日志中我们看出确实是成功地将状态保存了下来。\nstart 1c9de33e-84a0-4827-ae82-eae9dd8dec60 end 1c9de33e-84a0-4827-ae82-eae9dd8dec60 start 9c1f6eab-5c95-4a5f-b2f4-2f1a026684f5 start 61a32915-5316-4066-bfb0-382c41a2a1aa end 9c1f6eab-5c95-4a5f-b2f4-2f1a026684f5 end 61a32915-5316-4066-bfb0-382c41a2a1aa start bb1cb174-0f42-4dc6-9eab-85ccaef9d08c start 3ef04aba-dc0c-4c02-aebf-850dbf8c2e31 end bb1cb174-0f42-4dc6-9eab-85ccaef9d08c end 3ef04aba-dc0c-4c02-aebf-850dbf8c2e31 start a39d34be-da8a-44f6-b3b4-3eb6eb3501ef end a39d34be-da8a-44f6-b3b4-3eb6eb3501ef 这样的写法比起将logger作为参数在各个函数间传递的做法，可读性、可维护性更强，对业务代码的侵入也更低。\n以上是服务内日志追踪的办法。\n那如何做服务与服务之间的追踪呢？其实也很简单，将traceId注入到http-client的header中，或者更进一步，重写require函数，调用http/https模块时，将traceId注入。这里就不详细展开。\n","date":"2020-06-25T19:26:00+08:00","permalink":"https://blog.boluotou.tech/post/20200625-asynclocalstorage-and-logging/","title":"AsyncLocalStorage与日志追踪"},{"content":"2019年，虽然有很多不满意，但总的基调是好的。\n学习了诸多技术，包括TypeScript、Cocos、Haskell、OS、Akka和Vue。\nCocos就是个天坑，写这个，幸好从项目上下来了。另一位同事就没那么好运气了，直到现在还在写。\nVue作为后端开发的前端入门框架还是不错的，简单粗暴，出活快，但不够优雅。写前端实在不能称得上是什么有趣的事，光是调CSS一项就足够我崩溃了。花了那么多时间，就只为完成一个小小的样式，感觉并没有学到多少。写完Vue后，坚定了我不搞前端的决心。\nTypeScript是在开发Cocos中学习的语言，我简直爱死它了。强大的类型系统，模板元编程，工程友好，完美地符合我的要求。当然，如果不是用开发Cocos就更好了。在其他项目上也见识了有人用不用strict模式，any满天飞，看得我简直想打人了。\nHaskell学了一点，还没到Monad，但足以体会到其类型系统的强大，Scala相比之下感觉像个残次品。\n补了一点OS的知识。\nAkka是年末这几天才开始接触的，主要是新项目要用到。感觉Actor的理解真的很先进，但使用起来也很复杂，还不是很明白，新年继续学习。\n工作上顺利成为正式员工。这一年在四个项目上待过，现在回头一看这数量还真不少。\n现在回头看第一个项目还是比较简单的。在某种程度上帮助自己很平滑地适应了职场生活，是个好的开始。因为项目小，学了不少技术栈，扩展了视野，也锻炼了ops技能，总体而言，是个好的开始。后来在武汉过的也不错。年末时，抓住机会，成功进入目前的组，开始搞akka/reactor。\n生活上，终于走上正轨了，Nice！\n掌声送给自己，过去一年确实不容易。新年计划什么也没有，因为去年的计划根本就没看。\n","date":"2020-01-03T19:26:00+08:00","permalink":"https://blog.boluotou.tech/post/20200101-2019-review/","title":"2019年回顾"},{"content":"代码链接\ndemo如下，定义好rule后进行parse。\nconst rule = AndRule.of( [ TextRule.of(\u0026#34;hello\u0026#34;, \u0026#34;HELLO_RULE\u0026#34;), OrRule.of( [TextRule.of(\u0026#34;, \u0026#34;), TimesRule.of(3, TextRule.of(\u0026#34; \u0026#34;, \u0026#34;SPACE\u0026#34;), \u0026#34;TIMES\u0026#34;)], \u0026#34;NO_NAME\u0026#34; ), TextRule.of(\u0026#34;world\u0026#34;, \u0026#34;WORLD\u0026#34;), OneOrMoreRule.of(TextRule.of(\u0026#34;!\u0026#34;), \u0026#34;SAMPLE\u0026#34;) ], \u0026#34;HELLO_WORLD\u0026#34; ); console.log(JSON.stringify(rule.accept(\u0026#34;hello world!!!\u0026#34;))); 结果为：\n{ \u0026#34;contain\u0026#34;: true, \u0026#34;group\u0026#34;: { \u0026#34;groups\u0026#34;: [ { \u0026#34;text\u0026#34;: \u0026#34;hello\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;HELLO_RULE\u0026#34; }, { \u0026#34;groups\u0026#34;: [ { \u0026#34;groups\u0026#34;: [ { \u0026#34;text\u0026#34;: \u0026#34; \u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;SPACE\u0026#34; }, { \u0026#34;text\u0026#34;: \u0026#34; \u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;SPACE\u0026#34; }, { \u0026#34;text\u0026#34;: \u0026#34; \u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;SPACE\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;TIMES\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;NO_NAME\u0026#34; }, { \u0026#34;text\u0026#34;: \u0026#34;world\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;WORLD\u0026#34; }, { \u0026#34;groups\u0026#34;: [ { \u0026#34;text\u0026#34;: \u0026#34;!\u0026#34;, \u0026#34;name\u0026#34;: null }, { \u0026#34;text\u0026#34;: \u0026#34;!\u0026#34;, \u0026#34;name\u0026#34;: null }, { \u0026#34;text\u0026#34;: \u0026#34;!\u0026#34;, \u0026#34;name\u0026#34;: null } ], \u0026#34;name\u0026#34;: \u0026#34;SAMPLE\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;HELLO_WORLD\u0026#34; } } 当然，这离真正可用还有很远：\n","date":"2019-11-06T18:00:00+08:00","permalink":"https://blog.boluotou.tech/post/20191106-mini-parser/","title":"mini-parser小轮子"},{"content":"format: if\u0026lt;cond\u0026gt; branchByte1 branchByte2 从栈中弹出一个值，和0进行比较，根据指令的不同，有不同的比较方法得出一个值，如果为假，则顺序执行后面的指令。那为真的呢？：\nbranchByte1 branchByte2 都是 0x00 - 0xFF 之间的一个 unsigned 值，可以用他俩算出来offset作为true branch的入口，公式为：\n(branchbyte1 \u0026lt;\u0026lt; 8) | branchbyte2 例子：\n116: bipush 1 118: ifeq 132 121: getstatic #12 // Field java/lang/System.out:Ljava/io/PrintStream; 124: bipush 1 126: invokevirtual #28 // Method java/io/PrintStream.println:(Z)V 129: goto 140 132: getstatic #12 // Field java/lang/System.out:Ljava/io/PrintStream; 135: bipush 0 行中ifeq 132的二进制表示为99 00 0E。99代表该指令，00和0E代表两个操作数。\n计算偏移量：(0x00 \u0026lt;\u0026lt; 8) | 0x0E 得 14\n就是说ifeq开头为118，加上14个偏移量为132，所以132是 true branch 的入口。\n问题：\n一、为什么需要两个操作数呢？\n（一）通过观察 java 编译后的 class 文件，我发现 branchByte1 都是 0x00，完全没有必要为这样一个固定的值设计一个位置呀。\n自答：如果 true branch 非常长（true branch 里的指令长度超过了255，那么 branchByte1 是会增长的。\n（二）(branchbyte1 \u0026lt;\u0026lt; 8) | branchbyte2的结果还是8位，难道不可以用一个 byte 来表示，为什么需要用两个 byte 计算后得到结果呢？\n自答：如上所述，需要两个来处理 condition 很长的情况。\n","date":"2019-09-13T12:35:00+08:00","permalink":"https://blog.boluotou.tech/post/20190913-jvm-ifcond/","title":"JVM if\u003ccond\u003e指令笔记"},{"content":"之前经常变更学习方向，没有收到很好的学习效果，浪费了不少时间。最近痛定思痛，把方向定为JVM和编译原理，这次真的不改了。本文是学习该方向的阶段性总结。\n一、前言 之前写过几个解释器，但还没写过编译器。刚好看到知乎Belleve给出的一幅学习路线图，于是决定实现一个lisp方言的编译器。\n之所以选择JVM而不是X86作为目标平台，一是JVM平常用的多一些，可以互相印证、互相补充；二是文档和社区资源丰富友好，开发体验较好。\n项目地址：https://github.com/gcnyin/slisp\n截止最新的commit 77f126d4，实现的功能有：\n定义变量 支持字符串、整数和布尔类型 打印以上三种预置类型的值 四则运算 条件判断 二、编译和运行 来一段具体的Slisp程序：\n(define a (+ 1 2 3 4)) (println a) (define b (+ a a)) (println b) (define a (+ b b)) (println a) (println (+ (+ 1 1) (- 6 4) (* 2 2) (/ 4 2))) (println \u0026#34;Hello Slisp!\u0026#34;) (define c \u0026#34;Hello world!\u0026#34;) (println c) (println true) (println false) (define d true) (println d) (if true (println true) (println false)) (if (== 1 1) (println \u0026#34;1 == 1\u0026#34;) (println \u0026#34;1 != 1\u0026#34;)) 以上程序出自本项目/Slisp/Hello.slisp。\n想要运行必须先打包编译器：\n./gradlew clean build 得到了build/libs/slisp-0.1.0.jar，之后在命令行编译源代码：\njava -jar build/libs/slisp-0.1.0.jar Slisp/Hello.slisp 即可生成Hello.class文件，java Hello运行该文件，输出为：\n20 10 Hello Slisp! Hello world! true false true true == 1 三、编译器组成 这个编译器由三部分组成，一是前端部分，二是构建抽象语法树，三是递归下降生成字节码。\n前端部分使用了Antlr来构建。Antlr是一个流行的parser generator，可以根据给定的文法，生成相应的parser。因为Slisp本身采用了lisp系的语法，并不复杂，所以很容易写出文法供Antlr使用。\n构建抽象语法树使用了visitor模式。由于Antlr本身返回的结果已经是一棵树，所以这部分的工作是，根据每个节点不同的形态创建相应的类和实例。\n这里有一些实现上的细节可以优化，比如针对四则运算，可以将这些运算全部用一个类来表示，只更改其中的一个字段以示区别。还有一点是，如果打算只使用一个visitor，那么每个节点类都需要继承同一个接口或父类。\n还有，实现了一点简单的类型推导。传统的lisp方言大多是动态语言，不过Slisp是静态的，而且可以在定义变量时推导出变量的类型，不需要开发者手动声明变量的类型。(define a 123)、(define b \u0026ldquo;Hello\u0026rdquo;)和(define c true)可以由字面值推导出类型，而(define d (+ 1 (- 2 3))也可以推导出表达式(+ 1 (- 2 3))的类型并以此确定\u001dd的类型。\n生成字节码部分采用了递归下降来生成。比如对(+ (+ 1 1) (- 6 4) (* 2 2) (/ 4 2))，生成了：\n44: bipush 1 46: bipush 1 48: iadd 49: bipush 6 51: bipush 4 53: isub 54: iadd 55: bipush 2 57: bipush 2 59: imul 60: iadd 61: bipush 4 63: bipush 2 65: idiv 66: iadd 这段代码是Hello.class文件中的一部分，使用OpenJDK中的javap反汇编器生成。\n(+ 1 1)对应44、46和48，先将两个1压入栈中，然后相加，将之前的两个人从栈中弹出，然后将结果压入栈顶，继续执行(- 6 4)。\n这里需要注意的是，并不是说执行完这四个运算(+ 1 1) (- 6 4) (* 2 2) (/ 4 2)，然后再计算它们的和。而是在计算完(+ 1 1)和(- 6 4)之后（结果为2和2），立即计算了(+ 2 2)（得到4），然后计算(* 2 2)（得到4），再计算(+ 4 4)，以此类推。过程如下所示：\n(+ (+ 1 1) (- 6 4) (* 2 2) (/ 4 2)) (+ 2 (- 6 4) (* 2 2) (/ 4 2)) (+ 2 2 (* 2 2) (/ 4 2)) (+ 4 (* 2 2) (/ 4 2)) (+ 4 4 (/ 4 2)) (+ 8 (/ 4 2)) (+ 8 2) (10) 为了契合这样的字节码运算方式，后端在创建抽象语法树的时候需要注意“左结合与右结合”的问题。这里采用了右结合的方式，大致结构如下所示：\n(+ (/ 4 2) (+ (* 2 2) (+ (- 6 4) (+ 1 1)))) 这样从底层开始生成字节码，每生成一层，就向上传递，继续生成上层节点的字节码。\n实际开发中使用了ASM库来辅助生成字节码，只需要手动拼接好类似于bipush 1这样的文本传给ASM中合适的类和方法，最后调用generateBytecode这样的方法即可。\n虽然ASM库很方便，但想要生成符合语义的字节码，开发者仍需要阅读JVM规范。JVM规范中定义了各字节码的名称与语义，对照着网络上的众多示例还是很容易理解的。\n四、字节码简介 bipush是指将一个类型为byte扩充为int，然后压到栈上。\niadd是将栈最上面的两个int弹出，然后计算它们的和，将结果压入栈顶。imul、isub和idiv都类似于iadd，不同之处在于将运算符变为了*、-和/。\nistore将int保存在局部变量中。\niload从局部变量中取出保存在其中的值。\nastore是将对一个Ojbect的引用保存在局部变量中。\nalocal是将保存在局部变量中的引用压入栈顶。\nifeq是将栈顶的值与0进行比较，如果相等，进入true branch，否则进行false branch。该指令还会指定一个数字作为false branch入口的地址。\nif_icmpne是比较栈上的两个类型为int的值，如果不相等，进入true branch，否则进入false branch。\n值得注意的是，诸如if这样的指令并不是单个存在，它们更多的像是一个家庭，比如比较两个int会有许多相似的指令，从JVM规范中抄录一段：\n• if_icmpeq succeeds if and only if value1 = value2 • if_icmpne succeeds if and only if value1 ≠ value2 • if_icmplt succeeds if and only if value1 \u0026lt; value2 • if_icmple succeeds if and only if value1 ≤ value2 • if_icmpgt succeeds if and only if value1 \u0026gt; value2 • if_icmpge succeeds if and only if value1 ≥ value2 可以看到if_icmpne只是用来比较两个数相等时的情况，还有其它指令用于比较不等、大于、小于、相等时的情况。像这样相似而略有区别的指令，JVM规范大多将它们的文档合并在一起，并起名为if_icmp，这里的cond代表每个指令独特的部分。\n","date":"2019-01-01T21:00:00+08:00","permalink":"https://blog.boluotou.tech/post/20190101-slisp/","title":"slisp：一门简单的JVM上的Lisp方言"},{"content":"过年前拿到了offer，详见知乎。\n舍友们一起出去旅游，去了华哥他老家还有重庆。\n毕业前的几个月，一直一个人窝在北郊的屋子里，长胖了很多。\n工作培训压力很大，没有食欲，还导致了胃酸，睡不好。\n年底分手了。\n","date":"2019-01-01T08:00:00+08:00","permalink":"https://blog.boluotou.tech/post/20190101-2018-review/","title":"2018年回顾"},{"content":"项目地址：https://github.com/gcnyin/arcee\n为什么要做这样一个东西呢？不是有Antlr吗，Python下不是也有相应的bind吗？人类为什么又要再做一遍已经成熟了的东西呢？\n答案是不爽！\n之前刷 EOPL ，想用 Python 改写其中的玩具语言，重写了三四个后，感觉很别扭。教材里自带了一个parser，所以不用考虑解释器前端的东西，但我用Python改写时，由于没有可口的前端，写起来很不爽，每次写完后端，都只能自己用 Python 手敲一遍AST，真的很麻烦，所以我就萌生了自己写一个 parser generator 的想法。\n所以，就有 Arcee 。\n使用方法：\nInstall\n$ pip install Arcee Example\n首先创建grammar文件:\nKEYWORDS : let, if, zero, - NUMBER : \\d+(\\.\\d*)? ASSIGN : = SUBTRACTION : - RIGHT_BRACKET : ( COLON : , LETF_BRACKET : ) ID : [A-Za-z]+ SKIP : [ \\\\t]+ program : expression ; expression : zeroexp | diffexp | ifexp | varexp | letexp | constexp ; constexp : $NUMBER ; diffexp : \u0026#39;-\u0026#39; \u0026#39;(\u0026#39; expression \u0026#39;,\u0026#39; expression \u0026#39;)\u0026#39; ; zeroexp : \u0026#39;zero\u0026#39; \u0026#39;(\u0026#39; expression \u0026#39;)\u0026#39; ; ifexp : \u0026#39;if\u0026#39; expression \u0026#39;then\u0026#39; expression \u0026#39;else\u0026#39; expression ; varexp : $ID ; letexp : \u0026#39;let\u0026#39; $ID \u0026#39;=\u0026#39; expression \u0026#39;in\u0026#39; expression ; 在命令行里执行：\n$ arcee grammar \u0026gt; result.py result.py has three parts:\nToken from collections import namedtuple Token = namedtuple(\u0026#39;Token\u0026#39;, [\u0026#39;type\u0026#39;, \u0026#39;value\u0026#39;, \u0026#39;line\u0026#39;, \u0026#39;column\u0026#39;]) Program = namedtuple(\u0026#39;Program\u0026#39;, [\u0026#39;expression\u0026#39;]) # ... Lexer import re def tokenize(code): pass # ... Parser class Parser: def __init__(self, token_list): pass # ... def parse_expression(self): if xxx: self.parse_constexp() elif yyy: self.parse_diffexp() #... def parse_constexp(self): pass def parse_diffexp(self): pass def parse_zeroexp(self): pass def parse_ifexp(self): pass def parse_varexp(self): pass def parse_letexp(self): pass You can parse input such as: input = \u0026#39;\u0026#39;\u0026#39;let a = 0 in if zero(a) then -(a, 1) else -(a, 2)\u0026#39;\u0026#39;\u0026#39; tokens = list(tokenize(input)) parser = Parser(tokens) parser.parse_program() result is: result = Program( expression=Expression( nonterminal=Letexp( ID=Token(type=\u0026#39;ID\u0026#39;, value=\u0026#39;a\u0026#39;, line=2, column=4), expression1=Expression( nonterminal=Constexp( NUMBER=Token(type=\u0026#39;NUMBER\u0026#39;, value=\u0026#39;0\u0026#39;, line=2, column=8))), expression2=Expression( nonterminal=Ifexp( expression1=Expression( nonterminal=Zeroexp( expression=Expression( nonterminal=Varexp( ID=Token(type=\u0026#39;ID\u0026#39;, value=\u0026#39;a\u0026#39;, line=2, column=21))))), expression2=Expression( nonterminal=Diffexp( expression1=Expression( nonterminal=Varexp( ID=Token(type=\u0026#39;ID\u0026#39;, value=\u0026#39;a\u0026#39;, line=2, column=31))), expression2=Expression( nonterminal=Constexp( NUMBER=Token(type=\u0026#39;NUMBER\u0026#39;, value=\u0026#39;1\u0026#39;, line=2, column=34))))), expression3=Expression( nonterminal=Diffexp( expression1=Expression( nonterminal=Varexp( ID=Token(type=\u0026#39;ID\u0026#39;, value=\u0026#39;a\u0026#39;, line=2, column=44))), expression2=Expression( nonterminal=Constexp( NUMBER=Token(type=\u0026#39;NUMBER\u0026#39;, value=\u0026#39;2\u0026#39;, line=2, column=47)))))))))) 这样就获得了AST。\n这个轮子目前还有一点小问题，不过自己用的话还是没问题。由于工作缘故，估计是要去学 JavaScript 了，这个东西估计不会再更新了（也许哪天还会的。。。），到时估计就是重写一个 npm 包吧，这个再说。\n","date":"2018-09-02T12:35:00+08:00","permalink":"https://blog.boluotou.tech/post/20180902-arcee/","title":"Arcee：又一个Parser Generator轮子"}]